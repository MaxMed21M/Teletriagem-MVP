# Teletriagem ‚Äî MVP

Aplica√ß√£o de triagem m√©dica composta por um backend FastAPI e uma interface Streamlit.
O fluxo principal permite registrar triagens manuais e solicitar um resumo estruturado
gerado por um modelo via Ollama.

## Requisitos

- Python 3.12
- [Ollama](https://ollama.com/) em execu√ß√£o local
- Modelo `qwen3b_q4km:latest` baixado (`ollama pull qwen3b_q4km:latest`)

## Configura√ß√£o

1. Crie e ative um ambiente virtual.
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   ```

2. Instale as depend√™ncias.
   ```bash
   pip install -r requirements.txt
   ```

3. Configure as vari√°veis de ambiente (opcional). Caso n√£o exista, crie um arquivo `.env`
   na raiz com o seguinte conte√∫do m√≠nimo:
   ```env
   LLM_PROVIDER=ollama
   LLM_MODEL=qwen3b_q4km:latest
   OLLAMA_BASE_URL=http://127.0.0.1:11434
   ```

## Execu√ß√£o

O comando abaixo inicia a API (porta `8000`) e a UI Streamlit (porta `8501`).

```bash
python run_all.py
```

Use `python run_all.py --lite` para subir apenas a API.

## Endpoints principais

- `GET /health` ‚Üí status da aplica√ß√£o
- `POST /api/triage/` ‚Üí registra triagem manual
- `POST /api/triage/ai` ‚Üí gera triagem assistida por IA (contrato fixo: `prompt`, `model_text`, `parsed`, `parse_error`, `id`)
- `GET /llm/ollama/health` ‚Üí valida se o modelo configurado est√° dispon√≠vel
- `GET /api/glossary/search` *(quando `AI_GLOSSARIO=1`)* ‚Üí busca termos normalizados
- `POST /api/triage/{id}/review` *(quando `AI_HITL=1`)* ‚Üí registra aceita√ß√£o/override/rejei√ß√£o humana
- `GET /api/triage/{id}/export/pec` *(quando `AI_EXPORT_PEC=1`)* ‚Üí gera payload estruturado para PEC
- `GET /api/metrics/summary` *(quando `AI_METRICS`/`AI_DRIFT_BIAS` ativos)* ‚Üí estat√≠sticas de uso

A documenta√ß√£o interativa est√° em `http://127.0.0.1:8000/docs`.

## Interface Streamlit

A interface est√° em `http://127.0.0.1:8501` e oferece:

- formul√°rio compartilhado para triagens manuais e IA;
- abas ‚ÄúEstruturado‚Äù, ‚ÄúTexto do modelo‚Äù e ‚ÄúJSON bruto‚Äù para cada resposta da IA, com m√©tricas de confian√ßa, explica√ß√µes e CID-10 quando `AI_XAI`/`AI_STRICT_JSON` estiverem ativos;
- painel de debug opcional exibindo o payload enviado e a resposta recebida;
- campo na barra lateral para alterar a URL da API, consultar gloss√°rio popular (quando ativo) e visualizar m√©tricas.
- modo HITL opcional adiciona bot√µes para aceitar/override/rejeitar a triagem diretamente da UI.
- √°rea ‚ÄúüîÑ Refinar triagem‚Äù abaixo da resposta da IA para enviar complementos (gera nova vers√£o + audit trail).
- exibi√ß√£o de anexos, normaliza√ß√µes populares, contexto epidemiol√≥gico e hist√≥rico de vers√µes quando dispon√≠veis.
- painel epidemiol√≥gico simplificado (necessita `AI_METRICS=1`) com agregados semanais por queixa/munic√≠pio.

## Feature flags (vari√°veis opcionais)

Todas as novas funcionalidades s√£o **opt-in**. Defina as vari√°veis abaixo como `1`/`true` para ativ√°-las:

| Flag | Descri√ß√£o |
| --- | --- |
| `AI_STRICT_JSON` | Exige JSON estrito (schema fornecido) e tenta reparo autom√°tico. Se falhar, responde `422`. |
| `AI_XAI` | Solicita explica√ß√µes objetivas, perguntas de follow-up e flags de incerteza. |
| `AI_HITL` | Ativa revis√£o humana (Human-in-the-loop) com endpoint de decis√£o. |
| `AI_GLOSSARIO` | Normaliza termos populares (inclui `AI_GLOSSARIO_FILE` para carregar JSON/XLSX customizado). |
| `AI_EXPORT_PEC` | Disponibiliza exporta√ß√£o JSON compat√≠vel com PEC/LEDI. |
| `AI_METRICS` | Coleta m√©tricas operacionais (lat√™ncia, distribui√ß√£o de prioridade, overrides). |
| `AI_DRIFT_BIAS` | Estende as m√©tricas com checagens simples de drift/vieses. |
| `AI_DOUBLE_CHECK_ENABLED` | Executa um segundo passe do LLM para revisar omiss√µes e corrigir o JSON. |
| `AI_CONFIDENCE_ENABLED` | Calcula pontua√ß√µes de confian√ßa por campo e geral. |
| `AI_EPI_WEIGHTING_ENABLED` | Ajusta ranking de causas prov√°veis usando sinais epidemiol√≥gicos simples (regi√£o/esta√ß√£o). |
| `AI_MIN_CONFIDENCE` | Limiar (0‚Äì1) para disparar `fallback_notice` quando a confian√ßa geral ficar abaixo do valor. |
| `AI_LATENCY_WARN_MS` | Lat√™ncia m√°xima tolerada em ms antes de sinalizar `latency_warning` na resposta. |

Exemplo `.env` com as flags principais:

```env
AI_STRICT_JSON=1
AI_XAI=1
AI_GLOSSARIO=1
AI_HITL=1
AI_EXPORT_PEC=1
AI_METRICS=1
AI_DOUBLE_CHECK_ENABLED=1
AI_CONFIDENCE_ENABLED=1
AI_EPI_WEIGHTING_ENABLED=1
AI_MIN_CONFIDENCE=0.7
AI_LATENCY_WARN_MS=5000
```

Para carregar um gloss√°rio customizado defina `AI_GLOSSARIO_FILE=/caminho/para/glossario.xlsx` (ou `.json`).

## Testes r√°pidos

Scripts de verifica√ß√£o manual est√£o em `scripts/test_api.ps1`:

```powershell
./scripts/test_api.ps1
```

O script realiza chamadas para `/health`, `/api/triage/` e `/api/triage/ai` usando PowerShell.

Para a su√≠te automatizada (incluindo casos cr√≠ticos de triagem, refinamento e valida√ß√£o estrita) utilize:

```bash
pytest -q
```

## Troubleshooting

- **Modelo indispon√≠vel**: verifique `python run_all.py` e a rota `/llm/ollama/health`.
- **Portas em uso**: altere `API_PORT` ou `UI_PORT` via vari√°veis de ambiente.
- **Timeouts do LLM**: ajuste `LLM_CONNECT_TIMEOUT_S`, `LLM_READ_TIMEOUT_S`, etc., no `.env`.

## Deploy leve

Para uso em VPS local, recomenda-se:

- executar `uvicorn backend.app.main:app --host 0.0.0.0 --port 8000 --workers 1`;
- iniciar o Streamlit com `streamlit run ui/home.py --server.port 8501 --server.headless true`;
- manter Ollama ativo com o modelo `qwen3b_q4km:latest` previamente baixado.
